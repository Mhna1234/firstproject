# -*- coding: utf-8 -*-
"""MLOPtimization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h4gE_HPHLRNN26fS0-DBtPnfB5UC62Zb
"""

import numpy as np
import matplotlib.pyplot as plt
import cvxpy as cp

def generate_data(d, m, sigma_max, sigma_min, noise_level):
    np.random.seed(42)
    U, _ = np.linalg.qr(np.random.randn(m, m))
    V, _ = np.linalg.qr(np.random.randn(d, d))
    singular_values = np.linspace(sigma_min, sigma_max, min(m, d))
    A = U[:, :len(singular_values)] @ np.diag(singular_values) @ V[:len(singular_values), :]
    x_star = np.random.randn(d)
    noise = noise_level * np.random.randn(m)
    b = A @ x_star + noise
    return A, b, x_star


d = 100
m = 200
sigma_max = 10
sigma_min = 2
noise_level = 0.1

A, b, x_star = generate_data(d, m, sigma_max, sigma_min, noise_level)



def function(A,b):
  return lambda x: 0.5*np.linalg.norm(A @ x - b)**2

def grad_function(A,b):
  return lambda x: A.T @ (A @ x - b)

def subgradient_descent(A, b, eta, num_iterations):
    x = np.zeros(A.shape[1])
    f = function(A, b)
    grad_f = grad_function(A, b)
    values = [f(x)]
    x_bar=x

    for i in range(num_iterations):
        x = x - eta * grad_f(x)
        x_bar=((i+1)*x_bar+x)/(i+2)
        values.append(f(x_bar))
    return values

def gradient_descent_smooth(A, b, beta, num_iterations):
    x = np.zeros(A.shape[1])
    f = function(A, b)
    grad_f = grad_function(A, b)
    values = [f(x)]

    for _ in range(num_iterations):
        x = x - (1/beta) * grad_f(x)
        values.append(f(x))
    return values

def accelerated_gradient_descent(A, b, beta, num_iterations):
    x_t = np.zeros(A.shape[1])
    y_t = np.zeros(A.shape[1])
    eta=1
    f = function(A, b)
    grad_f = grad_function(A, b)
    values = [f(x_t)]

    for _ in range(num_iterations):
      z_t=(1-eta)*y_t+eta*x_t
      x_t=x_t-1/(beta*eta)*grad_f(z_t)
      y_t = (1-eta)*y_t+eta*x_t
      values.append(f(y_t))
      eta=0.5*((-1)*eta**2+np.sqrt(eta**4 +4*eta**2) )
    print(len(values))
    return values

def stochastic_gradient_descent(A, b, eta, num_iterations):
    x = np.zeros(A.shape[1])
    f = function(A, b)
    values = [f(x)]
    covariance_matrix = np.eye(len(x))
    grad_f = grad_function(A, b)
    for _ in range(num_iterations):
        sgf = np.random.multivariate_normal(grad_f(x), covariance_matrix, 1)
        x = x - eta * sgf[0]
        values.append(f(x))
    return values

def svrg(A, b, alpha, beta, num_iterations):
    grad_f_i = lambda x, i: A[i, :].T * (A[i, :] @ x - b[i])
    y = np.zeros(A.shape[1])
    f = function(A, b)
    grad_f = grad_function(A, b)
    values = [f(y)]
    eta = 1 / (10 * beta)
    k = 20 * beta / alpha
    t = 0
    while t < num_iterations:
        x = np.copy(y)
        gy = grad_f(y)
        y_tag = np.zeros(A.shape[1])
        t0=t
        while t<t0+k:
            i = np.random.randint(0, b.shape[0])
            x = x - eta * (grad_f_i(x, i) - grad_f_i(y, i) + gy)
            y_tag += x
            t += 1
        y = y_tag / k
        values.append(f(y))
    values.extend([f(y) for i in range(num_iterations - len(values) + 1)])
    return values

def randomized_coordinate_descent(A, b, eta, num_iterations):
    x = np.zeros(A.shape[1])
    f = function(A, b)
    values = [f(x)]
    for _ in range(num_iterations):
        i = np.random.randint(A.shape[1])
        grad_i = A[:, i].T @ (A @ x - b)
        x[i] = x[i] -eta * A.shape[1] * grad_i
        values.append(f(x))
    return values

num_iterations = 1000
eigenvalues = np.linalg.eigvalsh(A.T @ A)
beta = np.max(eigenvalues)  # Largest eigenvalue of A.T @ A
alpha = np.min(eigenvalues)  # Smallest eigenvalue of A.T @ A
print(beta)
print(alpha)
eta=1/num_iterations
# Run algorithms
subgrad_results = subgradient_descent(A, b, eta, num_iterations)
grad_smooth_results = gradient_descent_smooth(A, b,beta, num_iterations)
accel_grad_results = accelerated_gradient_descent(A, b, beta, num_iterations)
sgd_results = stochastic_gradient_descent(A, b,eta, num_iterations)
svrg_results = svrg(A, b, alpha,beta, num_iterations)
coord_desc_results = randomized_coordinate_descent(A, b, eta, num_iterations)

# Collect results
results = {
    'Subgradient Descent': subgrad_results,
    'Gradient Descent': grad_smooth_results,
    'Accelerated Gradient Descent': accel_grad_results,
    'SGD': sgd_results,
    'SVRG': svrg_results,
    'Coordinate Descent': coord_desc_results}

def plot_results(results, num_iterations):
    for algorithm, values in results.items():
        plt.plot(range(num_iterations+1), values, label=algorithm)
    plt.xlabel('Iterations')
    plt.ylabel('Function Value')
    plt.title('Convergence Comparison')
    plt.legend()
    plt.show()

plot_results(results, num_iterations)





